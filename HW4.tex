\documentclass{article}

\usepackage[margin=.75in]{geometry}
\usepackage{amsthm,mathtools}
\usepackage[USenglish]{babel}
\usepackage{hyperref, xurl}     % Used for better URL formatting
\usepackage{microtype} % Help text fit on the page more, rather than spilling into the margins
\usepackage{fancyhdr}
\usepackage[inline]{enumitem}
\usepackage{cleveref}
\usepackage{physics}
\usepackage{cancel}
\usepackage{halloweenmath}
\usepackage{unicode-math}

\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{pgfplots}
\usepackage{xfrac}
\usepackage{float}
\usepackage[newfloat]{minted}
\usepackage{xcolor, mdframed}

\usetikzlibrary{external}
\tikzexternalize[prefix=out/]

\setmathfont{latinmodern-math.otf}
\setmathfont{texgyrepagella-math.otf}[range=bb]

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

% Clickable link configuration
\hypersetup{
	linktoc = all,      % Have the entire TOC line link to the page
	pdfborder = {0 0 0} % Remove link borders
}

\crefname{enumi}{part}{parts}
\crefname{algocf}{algorithm}{algorithms}

\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem*{answer}{Answer}

\definecolor{lightgray}{gray}{0.95}
\newmintedfile[rustcode]{rust}{
	linenos,
	firstnumber=1,
	tabsize=2,
	% bgcolor=lightgray,
	% frame=single,
	breaklines,
	% texcomments % Turned off due to the presence of _ characters in many comments
	escapeinside = \$\$,
}
\setmonofont{DejaVu Sans Mono}

% Title page information
\title{Homework 4\\{\large Math 5424}\\{\large Numerical Linear Algebra}}
\author{Alexander Novotny\\Zuriah Quinton}

% Setup headers for page numbers
\pagestyle{fancy}
\rhead{Novotny \& Quinton}
\lhead{Math 5424 HW 4}

\pagenumbering{arabic} % Change page numbering to lowercase roman for pre-pages
%\xrightswishingghost{\phantom{text}}
\renewcommand{\qedsymbol}{$\bigpumpkin$}

\newcommand{\comp}  {\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\ints}  {\mathbb{Z}}
\newcommand{\mat}[1]{\symbfit{#1}}
\begin{document}

\maketitle

\begin{enumerate}[leftmargin=\labelsep]
	% 1
	\item Let \(\mat{L}\) be a lower triangular matrix and solve \(\mat{L}\vec{x} = \vec{b}\) by forward substitution. Show that barring overflow or underflow, the computed solution \(\hat{x}\) satisfies \((\mat{L} + \delta\mat{L})\hat{x} = \vec{b}\), where \(|\delta l_{ij}| \leq n\varepsilon|l_{ij}|\), where \(\varepsilon\) is the machine precision. This means that forward substitution is backward stable. Argue that backward substitution for solving upper triangular systems satisfies the same bound.
	      \begin{proof}
		      We have, for the true solution \(\vec{x}\),
		      \[
			      x_i = \qty(b_i - \sum_{k = 1}^{i-1} l_{ik}x_k) / l_{ii}.
		      \]
		      For the solution \(\hat{x}\) computed via backwards substitution,
		      \begin{align*}
			      \hat{x}_i & = \qty(b_i - \sum_{k = 1}^{i-1} \qty[l_{ik}\hat{x}_k(1 + \varepsilon^*_k) \prod_{j=k}^{i-1}(1 + \varepsilon^+_k)])(1 + \varepsilon^-)(1 + \varepsilon^/) / l_{ii}                                              \\
			      \shortintertext{(where \(|\varepsilon^*_k|, |\varepsilon^+_k|, |\varepsilon^-|, |\varepsilon^/| < eps\))}
			                & = \qty(b_i - \sum_{k = 1}^{i-1} \qty[l_{ik}(1 + \varepsilon^*_k) \prod_{j=k}^{i-1}(1 + \varepsilon^+_j)]\hat{x}_k) / \qty(\frac{l_{ii}}{(1 + \varepsilon^-)(1 + \varepsilon^/)})                               \\
			                & = \qty(b_i - \sum_{k = 1}^{i-1} \qty[l_{ik}\qty(1 + \varepsilon^*_k + \sum_{j=k}^{i-1}\varepsilon^+_j + \mathcal{O}(eps^2))]\hat{x}_k) / \qty(l_{ii}(1 - \varepsilon^- - \varepsilon^/ + \mathcal{O}(eps^2))).
		      \end{align*}
		      Then we have that \((\mat{L} + \delta\mat{L})\hat{x} = \vec{b}\) for
		      \begin{alignat*}{4}
			               &  & \delta l_{ij}   & = \begin{cases}
				                                        l_{ij} (- \varepsilon^- - \varepsilon^/),                       & \quad i = j    \\
				                                        l_{ij} \qty(\varepsilon^*_j + \sum_{k=j}^{i-1}\varepsilon^+_k), & \quad i \neq j
			                                        \end{cases}          \\
			               &  &                 & = l_{ij} \begin{cases}
				                                               - \varepsilon^- - \varepsilon^/,                   & \quad i = j    \\
				                                               \varepsilon^*_j + \sum_{k=j}^{i-1}\varepsilon^+_k, & \quad i \neq j
			                                               \end{cases}                \\
			      \implies &  & |\delta l_{ij}| & = |l_{ij}| \begin{cases}
				                                                 |(- \varepsilon^- - \varepsilon^/)|,                      & \quad i = j    \\
				                                                 \vqty{\varepsilon^*_j + \sum_{k=j}^{i-1}\varepsilon^+_k}, & \quad i \neq j
			                                                 \end{cases} \\
			      \shortintertext{(note that for \(i = 1,\ \varepsilon^- = 0\), since we subtract by 0 - which will never round)}
			               &  &                 & \leq |l_{ij}| \begin{cases}
				                                                    |\varepsilon^/|,                                       & \quad i = j = 1    \\
				                                                    |\varepsilon^-| + |\varepsilon^/|,                     & \quad i = j \neq 1 \\
				                                                    |\varepsilon^*_j| + \sum_{k=j}^{i-1}|\varepsilon^+_k|, & \quad i \neq j
			                                                    \end{cases}   \\
			               &  &                 & \leq |l_{ij}| \begin{cases}
				                                                    eps,        & \quad i = j = 1    \\
				                                                    2eps,       & \quad i = j \neq 1 \\
				                                                    (i-j+1)eps, & \quad i \neq j
			                                                    \end{cases}                                              \\
			               &  &                 & \leq n \cdot eps |l_{ij}|
		      \end{alignat*}
	      \end{proof}

	      % 2
	\item Matrix \(\mat{A}\) is called \emph{strictly column diagonally dominant}, or diagonally dominant for short, if
	      \begin{equation}
		      |\alpha_{ii}| > \sum_{j=1, j \neq i}^{n} |\alpha_{ji}| \label{eq:diagdom}
	      \end{equation}
	      Show that Gaussian elimination with partial pivoting does not actually permute any rows, i.e., that it is identical to Gaussian elimination without pivoting. Hint: Show that after one step of Gaussian elimination, the trailing \((n - 1)\text{-by-}(n - 1)\) submatrix, the \emph{Schur complement} of \(\alpha_{11}\) in \(\mat{A}\), is still diagonally dominant.
	      \begin{proof}
		      For a diagonally dominant matrix \(\mat{A}\), the first step of Gaussian elimination with partial pivoting does not permute any rows, since \(|\alpha_{ii}| > \sum_{j=1, j \neq i}^{n} |\alpha_{ji}|\) and thus \(|\alpha_{ii}| > |\alpha_{ji}|\) for any \(j\neq i\). Then, we have the decomposition for \(\mat{A}\) given by the first step of Gaussian elimination as
		      \begin{align*}
			      \mat{A} & = \mqty[\alpha_{11} & \vec{c}^{\top} \\ \vec{a} & \hat{\mat{A}}]                                                                          \\
			              & = \mqty[1           & 0              \\ \vec{l} & \mat{I}] \mqty[\alpha_{11} & \vec{c}^{\top} \\ 0 & \hat{\mat{A}}-\vec{l}\vec{c}^{\top}] \\
			              & = \mqty[1           & 0              \\ \vec{l} & \mat{I}] \mqty[\alpha_{11} & \vec{c}^{\top}                                             \\ 0 & \mat{B}]
		      \end{align*}
		      where \(\vec{l} = \frac{\vec{a}}{\alpha_{11}}\). Note that \(c_i = \alpha_{1,i + 1},\ l_i = \frac{\alpha_{i + 1, 1}}{\alpha_{11}}\). Then, denote the submatrix \(\mat{B}=\hat{\mat{A}}-\vec{l}\vec{c}^{\top}\). Now, it is sufficient to show that \(\mat{B}\) is diagonally dominant, since then we could continue each step of Gaussian elimination recursively always getting a diagonally dominant submatrix. First note
		      \begin{equation}
			      \beta_{ji} = \alpha_{j+1,i+1} - \frac{\alpha_{j+1,1}\alpha_{1,i+1}}{\alpha_{11}} \label{eq:bji}
		      \end{equation}
		      is the expression for the elements in \(\mat{B}\) by definition. Further, from \cref{eq:diagdom} we can split one of the terms out of the sum, and we have
		      \begin{equation}
			      \abs{\alpha_{ii}} - \sum_{\substack{j=1\\j\neq i,k}} \abs{\alpha_{ji}} > \abs{\alpha_{ki}}. \label{eq:diagdomk}
		      \end{equation}
		      We have
			      {
				      \allowdisplaybreaks
				      \begin{align*}
					      \abs{\beta_{ii}} & \stackrel{\mathclap{\eqref{eq:bji}}}{=} \abs{\alpha_{i+1,i+1} - \frac{\alpha_{i+1,1}\alpha_{1,i+1}}{\alpha_{11}}}                                                                                                                                                   \\
					                       & \geq \abs{\alpha_{i+1,i+1}} - \abs{\frac{\alpha_{i+1,1}\alpha_{1,i+1}}{\alpha_{11}}} \qquad\text{by reverse triangle inequality}                                                                                                                                    \\
					                       & \stackrel{\mathclap{\eqref{eq:diagdom}}}{>} \sum_{\substack{j=1\\j\neq i+1}}^n \abs{\alpha_{j,i+1}} - \abs{\frac{\alpha_{i+1,1}\alpha_{1,i+1}}{\alpha_{11}}}                                                                                                        \\
					                       & \stackrel{\mathclap{\eqref{eq:diagdomk}}}{>} \sum_{\substack{j=1\\j\neq i+1}}^n \abs{\alpha_{j,i+1}} - \frac{\abs{\alpha_{1,i+1}}}{\abs{\alpha_{11}}}\qty( \abs{\alpha_{11}} - \sum_{\substack{j=1\\j\neq 1,i+1}}^n \abs{\alpha_{j,1}})                             \\
					                       & = \sum_{\substack{j=1\\j\neq i+1}}^n \abs{a_{j,i+1}} - \abs{\alpha_{1,i+1}} + \frac{\abs{\alpha_{1,i+1}}}{\abs{\alpha_{11}}} \sum_{\substack{j=1\\j\neq 1,i+1}}^n \abs{\alpha_{j,1}}                                                                                \\
					                       & = \overbrace{\cancel{\abs{a_{1,i+1}}}+\sum_{\substack{j=2\\j\neq i+1}}^n \abs{a_{j,i+1}}}^{\text{split }j=1\text{ term out}} - \cancel{\abs{\alpha_{1,i+1}}} + \frac{\abs{\alpha_{1,i+1}}}{\abs{\alpha_{11}}} \sum_{\substack{j=2\\j\neq i+1}}^n \abs{\alpha_{j,1}} \\
					                       & = \sum_{\substack{j=2\\j\neq i+1}}^n \abs{a_{j,i+1}} + \frac{\abs{\alpha_{1,i+1}}}{\abs{\alpha_{11}}} \sum_{\substack{j=2\\j\neq i+1}}^n \abs{\alpha_{j,1}}                                                                                                         \\
					                       & = \sum_{\substack{j=2\\j\neq i+1}}^n \qty(\abs{a_{j,i+1}} + \frac{\abs{\alpha_{1,i+1}}\abs{\alpha_{j,1}}}{\abs{\alpha_{11}}})                                                                                                                                       \\
					                       & \geq \sum_{\substack{j=2\\j\neq i+1}}^n \qty(a_{j,i+1} - \frac{\alpha_{1,i+1}\alpha_{j,1}}{\alpha_{11} })                                                                                                                                                           \\
					                       & \stackrel{\mathclap{\eqref{eq:bji}}}{=}  \sum_{\substack{j=1\\j\neq i}}^{n-1} \abs{\beta_{ji}}.                                                                                                                                                                     \\
				      \end{align*}}
		      Thus, \(\mat{B}\) is diagonally dominant. Then, by induction each iteration of Gaussian elimination on a diagonally dominant matrix produces a diagonally dominant submatrix, so no pivoting is required.
	      \end{proof}

	      % 3
	\item Let \(\mat{A},\ \mat{B}\), and \(\mat{C}\) be matrices with dimensions such that the product \(\mat{A}^\top\mat{C}\mat{B}^\top\) is well defined. Let \(\mathcal{X}\) be the set of matrices \(\mat{X}\) minimizing \(\|\mat{A}\mat{X} \mat{B} - \mat{C}\|_F\), and let \(\mat{X}_0\) be the unique member of \(\mathcal{X}\) minimizing \(\|\mat{X}\|_F\). Show that \(\mat{X}_0 = \mat{A}^+C\mat{B}^+\). Hint: Use the SVDs of \(\mat{A}\) and \(\mat{B}\).
	      \begin{proof}

	      \end{proof}

	      % 4
	\item Show that the Moore—Penrose pseudoinverse of \(\mat{A}\) satisfies the following identities:
	      \begin{align*}
		      \mat{A}\mat{A}^+\mat{A}   & = \mat{A},                 \\
		      \mat{A}^+\mat{A}\mat{A}^+ & = \mat{A}^+,               \\
		      \mat{A}^+\mat{A}          & = (\mat{A}^+\mat{A})^\top, \\
		      \mat{A}\mat{A}^+          & = (\mat{A}\mat{A}^+)^\top.
	      \end{align*}
	      \begin{proof}
		      Assuming we have a full rank matrix \(\mat{A}\) with more rows than columns, \(\mat{A}^+=\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\), and we have
		      \begin{align*}
			      \mat{A}\mat{A}^+\mat{A} & = \mat{A}\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\mat{A}                          \\
			                              & = \mat{A}\cancel{\qty(\mat{A}^{\top}\mat{A})^{-1}}\cancel{\qty(\mat{A}^{\top}\mat{A}) } \\
			                              & = \mat{A}.
		      \end{align*}
		      Then,
		      \begin{align*}
			      \mat{A}^+\mat{A}\mat{A}^+ & = \qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\mat{A}\mat{A}^+                     \\
			                                & = \cancel{\qty(\mat{A}^{\top}\mat{A})^{-1}}\cancel{(\mat{A}^{\top}\mat{A})}\mat{A}^+ \\
			                                & = \mat{A}^+.
		      \end{align*}
		      Further,
		      \begin{align*}
			      \mat{A}^+\mat{A} & = \qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\mat{A}                          \\
			                       & = \cancel{\qty(\mat{A}^{\top}\mat{A})^{-1}}\cancel{\qty(\mat{A}^{\top}\mat{A}) } \\
			                       & = \mat{I},
		      \end{align*}
		      and
		      \begin{align*}
			      (\mat{A}^+\mat{A})^\top & = \mat{A}^{\top} (\mat{A}^+)^{\top}                                                \\
			                              & = \mat{A}^{\top} (\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top})^{\top}           \\
			                              & = \mat{A}^{\top} \mat{A} \qty(\qty(\mat{A}^{\top}\mat{A})^{-1})^{\top}             \\
			                              & = \mat{A}^{\top} \mat{A} \qty(\qty(\mat{A}^{\top}\mat{A})^{\top})^{-1}             \\
			                              & = \cancel{\qty(\mat{A}^{\top} \mat{A})} \cancel{\qty(\mat{A}^{\top} \mat{A})^{-1}} \\
			                              & = \mat{I},
		      \end{align*}
		      so \(\mat{A}^+\mat{A}= (\mat{A}^+\mat{A})^\top\).
		      Finally,
		      \begin{align*}
			      (\mat{A}\mat{A}^+)^\top & = \qty(\mat{A}^+)^{\top}\mat{A}^\top                                  \\
			                              & = (\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top})^{\top}\mat{A}^\top \\
			                              & = \mat{A}\qty(\qty(\mat{A}^{\top}\mat{A})^{-1})^{\top}\mat{A}^\top    \\
			                              & = \mat{A}\qty(\qty(\mat{A}^{\top}\mat{A})^{\top})^{-1}\mat{A}^\top    \\
			                              & = \mat{A}\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^\top                 \\
			                              & =\mat{A}\mat{A}^+.
		      \end{align*}
		      Then for a full rank matrix with more columns than rows, \(\mat{A}^+ = \mat{A}^\top\qty(\mat{A}\mat{A}^\top)^{-1}\), and we have
		      \begin{align*}
			      \mat{A}\mat{A}^+\mat{A} & = \mat{A}\mat{A}^\top\qty(\mat{A}\mat{A}^\top)^{-1}\mat{A}                               \\
			                              & = \cancel{\qty(\mat{A}\mat{A}^{\top})}\cancel{\qty(\mat{A}\mat{A}^{\top})^{-1} } \mat{A} \\
			                              & = \mat{A}.
		      \end{align*}
		      Then,
		      \begin{align*}
			      \mat{A}^+\mat{A}\mat{A}^+ & = \mat{A}^+\mat{A}\mat{A}^\top\qty(\mat{A}\mat{A}^\top)^{-1}                         \\
			                                & = \mat{A}^+\cancel{\qty(\mat{A}\mat{A}^{\top})}\cancel{(\mat{A}\mat{A}^{\top})^{-1}} \\
			                                & = \mat{A}^+.
		      \end{align*}
		      Further,
		      \begin{align*}
			      (\mat{A}^+\mat{A})^\top & = \qty(\qty(\mat{A}^\top(\mat{A}\mat{A}^\top)^{-1})\mat{A})^{\top} \\
			                              & = \mat{A}^\top \qty(\mat{A}^\top(\mat{A}\mat{A}^\top)^{-1})^\top   \\
			                              & = \mat{A}^\top \qty((\mat{A}\mat{A}^\top)^{-1})^\top \mat{A}       \\
			                              & = \mat{A}^\top \qty((\mat{A}\mat{A}^\top)^\top)^{-1} \mat{A}       \\
			                              & = \mat{A}^\top \qty(\mat{A}\mat{A}^\top)^{-1} \mat{A}              \\
			                              & = \mat{A}^+\mat{A}.
		      \end{align*}
		      Finally,
		      \begin{align*}
			      (\mat{A}\mat{A}^+)^\top & = \qty(\mat{A}\mat{A}^\top(\mat{A}\mat{A}^\top)^{-1})^\top \\
			                              & = \mat{I},
		      \end{align*}
		      and
		      \begin{align*}
			      \mat{A}\mat{A}^+ & = \mat{A}\mat{A}^\top(\mat{A}\mat{A}^\top)^{-1} \\
			                       & = \mat{I},
		      \end{align*}
		      so \(\mat{A}\mat{A}^+ = (\mat{A}\mat{A}^+)^\top\).
	      \end{proof}

	      % 5
	\item
	      \begin{enumerate}
		      \item Describe a variant of Gaussian elimination that introduces zeros into the columns of \(\mat{A}\) in the order \(n\) : -1 : 2 and which produces the factorization \(\mat{A} = \mat{U}\mat{L}\) where \(\mat{U}\) is the unit upper triangular and \(\mat{L}\) is lower triangular.
		            \begin{answer}
			            Note that for a \(1 \times 1\) matrix \(\mat{A}\), we have \(\mat{A} = \mat{I}\mat{A}\), where \(\mat{I}\) is unit upper triangular, and \(\mat{A}\) is lower triangular. Then for an \(n \times n\) matrix \(\mat{A}\), we have
			            \begin{align}
				            \mat{A} & = \mqty[\hat{\mat{A}} & \vec{a} \\\vec{c}^\top & \alpha_{nn}]                                        \\
				                    & = \mqty[\mat{I}       & \vec{l} \\\vec{0}^\top & 1]\mqty[\hat{\mat{A}}-\vec{l}\vec{c}^\top & \vec{0} \\\vec{c}^\top&\alpha_{nn}],
			            \end{align}
			            for \(\vec{l} = \frac{\vec{a}}{\alpha_{11}}\). Assume, by induction, that \(\hat{\mat{A}}-\vec{l}\vec{c}^\top = \mat{U}_1\mat{L}_1\) for some \(n-1 \times n-1\) unit upper triangular matrix \(\mat{U}_1\) and lower triangular matrix \(\mat{L}_1\). Then we have
			            \begin{align*}
				            \mat{A} & = \mqty[\mat{I}   & \vec{l} \\\vec{0}^\top & 1]\mqty[\mat{U}_1\mat{L}_1 & \vec{0}\\\vec{c}^\top & \alpha_{nn}] \\
				                    & = \mqty[\mat{U}_1 & \vec{l} \\\vec{0}^\top & 1]\mqty[\mat{L}_1          & \vec{0}\\\vec{c}^\top & \alpha_{nn}] \\
				                    & = \mat{U}\mat{L},
			            \end{align*}
			            where \(\mat{U}\) is the unit upper triangular and \(\mat{L}\) is lower triangular. This produces a recursive algorithm, which can be seen in \cref{alg:ul-recursive}.

			            \begin{algorithm}
				            \caption{A recursive algorithm to factorize \(\mat{A} = \mat{U}\mat{L}\) where \(\mat{U}\) is unit upper triangular and \(\mat{L}\) is lower triangular.}
				            \label{alg:ul-recursive}
				            \KwData{\(\mat{A}\)}
				            \KwResult{\(\mat{U}, \mat{L}\)}
				            \(\vec{l} \gets \vec{a} / \alpha_{11}\)\;
				            Factorize \(\hat{\mat{A}} - \vec{l}\vec{c}^\top = \mat{U}_1\mat{L_1}\)\;
				            \(\mat{U} \gets \mqty[\mat{U}_1 & \vec{l} \\\vec{0}^\top&1]\)\;
				            \(\mat{L} \gets \mqty[\mat{L}_1&\vec{0}\\\vec{c}^\top&\alpha_{nn}]\)\;
			            \end{algorithm}

			            If we pre-allocate space for the entirety of \(\mat{U}, \mat{L}\), this algorithm is tail-recursive, and can be de-recursed. As well, only the entries above the diagonal of \(\mat{U}\) are modified, and only the entries on and below the diagonal of \(\mat{L}\) are modified - so they can be stored in the same matrix. This leads to \cref{alg:ul-iterative}.

			            \begin{algorithm}
				            \caption{An iterative algorithm to factorize \(\mat{A} = \mat{U}\mat{L}\) where \(\mat{U}\) is unit upper triangular and \(\mat{L}\) is lower triangular.}
				            \label{alg:ul-iterative}
				            \KwData{\(\mat{A}\)}
				            \KwResult{\(\mat{UL}\) - a single matrix which stores the entries of \(\mat{U}\) above the diagonal and the entries of \(\mat{L}\) below the diagonal.}
				            \(\mat{UL} \gets \mat{A}\)\;
				            \For{$i\leftarrow n$ \KwTo $2$}{
					            \(\mat{UL}[1:i-1,\ i] \gets \mat{UL}[1:i-1,\ i] / \mat{UL}_{ii}\)\;
					            \tcc{Note that \(\mat{UL}[1:i-1,\ 1:i-1]\) is unused for the solution so far - we can use this to store \(\hat{\mat{A}} - \vec{l}\vec{c}^\top\).}
					            \(\mat{UL}[1:i-1,\ 1:i-1] \gets \mat{UL}[1:i-1,\ 1:i-1] - \mat{UL}[1:i-1,\ i] \cdot \mat{UL}[i,\ 1:i-1]\);
				            }
			            \end{algorithm}

			            Note that in the case where it is not necessary to preserve \(\mat{A}\), we can use \(\mat{UL} = \mat{A}\)
		            \end{answer}

		      \item Based on your algorithm, prove/provide the necessary and sufficient determinant conditions for the existence of the UL decomposition.
		            \begin{answer}
			            \begin{theorem}
				            The following two statements are equivalent:
				            \begin{enumerate}
					            \item There exist a unique unit upper triangular matrix \(\mat{U}\) and nonsingular lower triangular matrix \(\mat{L}\) such that \(\mat{A} = \mat{U}\mat{L}\).
					            \item All trailing principal submatrices of \(\mat{A}\) are nonsingular.
				            \end{enumerate}
			            \end{theorem}
			            \begin{proof}\hfill
				            \begin{itemize}
					            \item[\(\implies\)] For a trailing submatrix \(\mat{A}_{ii}\), we have
						            \[
							            \mat{A} = \mqty[\mat{A}_{11}&\mat{A}_{1i}\\\mat{A}_{i1}&\mat{A}_{ii}] = \mqty[\mat{U}_{11}&\mat{U}_{1i}\\\mat{0}&\mat{U}_{ii}] \mqty[\mat{L}_{11}&\mat{0}\\\mat{L}_{i1}&\mat{L}_{ii}] = \mat{U}\mat{L}.
						            \]
						            Then \(\det(\mat{A}_{ii}) = \det(\mat{U}_{ii}\mat{L}_{ii}) = \det(\mat{U}_{ii})\det(\mat{L}_{ii}) = 1 \cdot \prod_{k=1}^i (\mat{L}_{ii})_{kk} \neq 0\), since \(\mat{U}\) is unit upper triangular and \(\mat{L}\) is nonsingular lower triangular.
					            \item[\(\impliedby\)] Note that this is trivially true for a \(1 \times 1\) matrix. Then, by induction, assume there exist a unique unit upper triangular matrix \(\mat{U}\) and nonsingular lower triangular matrix \(\mat{L}\) such that \(\mat{A} = \mat{U}\mat{L}\) for all \(n-1 \times n-1\) matrices with all trailing principal submatrices nonsingular. For
						            \begin{align*}
							            \mat{A}       & = \mqty[\hat{\mat{A}} & \vec{a} \\\vec{c}^\top & \alpha_{nn}],               \\
							            \shortintertext{and}
							            \hat{\mat{A}} & = \mat{U}_1\mat{L}_1,                                                        \\
							            \shortintertext{which exist by assumption, we have}
							            \mat{A}       & = \mqty[\mat{U}_1     & \vec{l} \\\vec{0}^\top & 1]\mqty[\mat{L}_1 & \vec{0} \\\vec{c}^\top&\alpha_{nn}],
						            \end{align*}
						            where \(\vec{l} = \vec{a} / \alpha_{nn}\). Note, then that \(0 \neq \det(\mat{A}) = \det(\mat{L}_1) \alpha_{nn}\), so \(\alpha_{nn} \neq 0\), and the lower triangular matrix is nonsingular.
				            \end{itemize}
			            \end{proof}
		            \end{answer}

		      \item Write a Matlab code to implement the UL decomposition and apply it to
		            \[
			            \mqty[1&0&2&1\\-4&5&3&-1\\-1&3&1&1\\0&2&0&1]
		            \]
		            to verify that your code generates the required decomposition \(\mat{A} = \mat{U}\mat{L}\).
		            \begin{answer}
			            The output for this matrix can be found below.
			            \begin{mdframed}[backgroundcolor=lightgray]
				            \setlength{\lineskip}{0pt}
				            \inputminted{text}{out/output5.txt}
			            \end{mdframed}
			            The code used to implement the algorithm and verify the given test matrix can be found below.
			            \begin{mdframed}[backgroundcolor=lightgray]
				            \rustcode[fontsize=\footnotesize]{src/prob5.rs}
			            \end{mdframed}
		            \end{answer}
	      \end{enumerate}

	      % 6
	\item Even though we rarely need to compute the inverse of a matrix, let us think about it in this problem. Let \(\mat{A} \in \reals^{n\times n}\) be an invertible matrix. Describe an algorithm (based on the LU Decomposition/Gaussian Elimination) that computes \(\mat{A}^{-1}\) with an operation count of \(8n^3/3\) flops (ignoring the lower order terms).
	      \begin{answer}
		      To compute the inverse of a matrix from the LU decomposition, we must solve two triangular systems \(n\) times. This is to find the \(n\) columns of \(\mat{A}^{-1}\) from the LU decomposition. Thus, the cost to perform this backward substitution is \(2n\) times the cost to solve a single triangular system, which is \(n^2\). Summing this together, the cost of finding \(\mat{A}^{-1}\) is \(\frac23 n^3 + 2n(n^2) = \frac83 n^3 + \mathcal{O}(n^2)\). The algorithm is described in more detail in \cref{alg:inverse}.
		      \begin{algorithm}
			      \caption{An algorithm to compute the inverse of a matrix \(\mat{A}\) using the LU decomposition.}
			      \label{alg:inverse}
			      \KwData{\(\mat{A}\)}
			      \KwResult{\(\mat{A}^{-1}\)}
			      Factorize \(\mat{A}=\mat{LU}\)\;
			      \tcc{LU decomposition costs \(\sfrac23\ n^3\).}
			      \For{$i\leftarrow 1$ \KwTo $n$}{
				      Solve \(\mat{LU}\vec{b}_i=\vec{e}_i\)\;
				      \tcc{Solving a linear equation from LU decomposition costs \(2\ n^2\).}
			      }
			      \(\mat{A}^{-1} \gets [\vec{b}_1\ \vec{b}_2\ \cdots\ \vec{b}_n]\);
		      \end{algorithm}
	      \end{answer}

	      % 7
	\item Inspired by the presentation in [Trefethen and Bau, SIAM Press, 1997], in this problem, we will numerically investigate the growth factor in LU with partial pivoting. In the class (see October 13th notes), we showed that the growth factor \(\rho_{pp}\) could be as large as \(\rho_{pp} = 2n-1\). Indeed we had found an example where this upper bound is attained. However, as we mentioned, the algorithm behaves much better in practice. Here, we will try LU with partial pivoting on random matrices with varying dimensions and plot the observations.

	      Use the command \texttt{n = ceil(logspace(1, 3, 1000))} to create (approximately) logarithmically spaced matrix dimensions between 10 and 1000. Some of the dimensions will be repeated. The variable \texttt{n} is a vector of size 1000 with entries ranging from 10 to 1000. Then, for every entry \texttt{n(i)} of \texttt{n}, i.e., for \(i = 1, 2, \dots , 1000\), create a random matrix \(\mat{A}\) using \texttt{A= randn(n(i), n(i))/sqrt(n(i))}. So, we are creating a random matrix of varying dimensions with normally distributed entries having mean zero and standard deviation \(\sqrt{n(i)}\). Then, compute the growth factor \(\rho_{pp}\) for every \(\mat{A}\). At the end you will have a vector of size 1000 whose entries corresponding to the growth factor for every random \(\mat{A}\). Using the \texttt{loglog} command (logarithmic scale both in the horizontal and 2 vertical axes), plot the growth factor vs the matrix dimensions \texttt{n}. On the same plot, by using the \texttt{hold on} command, plot the growth rate of \(\sqrt{n}\). How is the observed/numerical growth behaving with respect to the theoretical upper bound \(2^{n-1}\) and with respect to \(\sqrt{n}\) ? Comment on your observations.
	      \begin{answer}
		      The growth rate of the calculated \(\rho_{pp}\) v.s. \(\sqrt{n}\) can be found in \cref{fig:rhopp}. Note that \(\sqrt{n}\) has been scaled to more easily observe the difference in growth rates. It can be seen that \(\rho_{pp}\) grows slightly faster than \(\sqrt{n}\) on average, as expected (the average growth rate should be somewhere between \(\sqrt{n}\) and \(n^{\sfrac{3}{4}}\)).
		      \begin{figure}[H]
			      \centering
			      \begin{tikzpicture}
				      \begin{loglogaxis}[
						      width = .9\textwidth,
						      height = .4\textwidth,
						      legend pos = north west,
						      xlabel = {\(n\)},
						      set layers,
						      mark layer = axis tick labels,
					      ]
					      \addplot+[only marks] table [
							      y expr = \thisrowno{1},
							      x expr = \thisrowno{0},
						      ] {out/output7.txt};
					      \addlegendentry{\(\rho_{pp}\)}
					      \addplot+ [domain=10:1000, no marks,very thick ] {sqrt(x)/2.0};
					      \addlegendentry{\(\mathcal{O}\qty(\sqrt{n})\)}
				      \end{loglogaxis}
			      \end{tikzpicture}
			      \caption{Growth rate of \(\rho_{pp}\) vs. \(\sqrt{n}\).}\label{fig:rhopp}
		      \end{figure}

		      The code used for this problem can be found below.
		      \begin{mdframed}[backgroundcolor=lightgray]
			      \rustcode[fontsize=\footnotesize]{src/prob7.rs}
		      \end{mdframed}
	      \end{answer}
\end{enumerate}

\end{document}