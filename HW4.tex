\documentclass{article}

\usepackage[margin=.75in]{geometry}
\usepackage{amsthm,mathtools}
\usepackage[USenglish]{babel}
\usepackage{hyperref, xurl}     % Used for better URL formatting
\usepackage{microtype} % Help text fit on the page more, rather than spilling into the margins
\usepackage{fancyhdr}
\usepackage[inline]{enumitem}
\usepackage{cleveref}
\usepackage{physics}
\usepackage{cancel}
\usepackage{halloweenmath}
\usepackage{unicode-math}

\setmathfont{latinmodern-math.otf}
\setmathfont{texgyrepagella-math.otf}[range=bb]

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

% Clickable link configuration
\hypersetup{
	linktoc = all,      % Have the entire TOC line link to the page
	pdfborder = {0 0 0} % Remove link borders
}

\crefname{enumi}{part}{parts}
\crefname{algocf}{algorithm}{algorithms}

\theoremstyle{definition}
\newtheorem*{answer}{Answer}

% Title page information
\title{Homework 4\\{\large Math 5424}\\{\large Numerical Linear Algebra}}
\author{Alexander Novotny\\Zuriah Quinton}

% Setup headers for page numbers
\pagestyle{fancy}
\rhead{Novotny \& Quinton}
\lhead{Math 5424 HW 4}

\pagenumbering{arabic} % Change page numbering to lowercase roman for pre-pages
%\xrightswishingghost{\phantom{text}}
\renewcommand{\qedsymbol}{$\bigpumpkin$}

\newcommand{\comp}  {\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\ints}  {\mathbb{Z}}
\newcommand{\mat}[1]{\symbfit{#1}}

\begin{document}

\maketitle

\begin{enumerate}[leftmargin=\labelsep]
	% 1
	\item Let \(\mat{L}\) be a lower triangular matrix and solve \(\mat{L}\vec{x} = \vec{b}\) by forward substitution. Show that barring overflow or underflow, the computed solution \(\hat{x}\) satisfies \((\mat{L} + \delta\mat{L})\hat{x} = \vec{b}\), where \(|\delta l_{ij}| \leq n\varepsilon|l_{ij}|\), where \(\varepsilon\) is the machine precision. This means that forward substitution is backward stable. Argue that backward substitution for solving upper triangular systems satisfies the same bound.
	      \begin{proof}
		      We have, for the true solution \(\vec{x}\),
		      \[
			      x_i = \qty(b_i - \sum_{k = 1}^{i-1} l_{ik}x_k) / l_{ii}.
		      \]
		      For the solution \(\hat{x}\) computed via backwards substitution,
		      \begin{align*}
			      \hat{x}_i & = \qty(b_i - \sum_{k = 1}^{i-1} \qty[l_{ik}\hat{x}_k(1 + \varepsilon^*_k) \prod_{j=k}^{i-1}(1 + \varepsilon^+_k)])(1 + \varepsilon^-)(1 + \varepsilon^/) / l_{ii}                                              \\
			      \shortintertext{(where \(|\varepsilon^*_k|, |\varepsilon^+_k|, |\varepsilon^-|, |\varepsilon^/| < eps\))}
			                & = \qty(b_i - \sum_{k = 1}^{i-1} \qty[l_{ik}(1 + \varepsilon^*_k) \prod_{j=k}^{i-1}(1 + \varepsilon^+_j)]\hat{x}_k) / \qty(\frac{l_{ii}}{(1 + \varepsilon^-)(1 + \varepsilon^/)})                               \\
			                & = \qty(b_i - \sum_{k = 1}^{i-1} \qty[l_{ik}\qty(1 + \varepsilon^*_k + \sum_{j=k}^{i-1}\varepsilon^+_j + \mathcal{O}(eps^2))]\hat{x}_k) / \qty(l_{ii}(1 - \varepsilon^- - \varepsilon^/ + \mathcal{O}(eps^2))).
		      \end{align*}
		      Then we have that \((\mat{L} + \delta\mat{L})\hat{x} = \vec{b}\) for
		      \begin{alignat*}{4}
			               &  & \delta l_{ij}   & = \begin{cases}
				                                        l_{ij} (- \varepsilon^- - \varepsilon^/),                       & \quad i = j    \\
				                                        l_{ij} \qty(\varepsilon^*_j + \sum_{k=j}^{i-1}\varepsilon^+_k), & \quad i \neq j
			                                        \end{cases}          \\
			               &  &                 & = l_{ij} \begin{cases}
				                                               - \varepsilon^- - \varepsilon^/,                   & \quad i = j    \\
				                                               \varepsilon^*_j + \sum_{k=j}^{i-1}\varepsilon^+_k, & \quad i \neq j
			                                               \end{cases}                \\
			      \implies &  & |\delta l_{ij}| & = |l_{ij}| \begin{cases}
				                                                 |(- \varepsilon^- - \varepsilon^/)|,                      & \quad i = j    \\
				                                                 \vqty{\varepsilon^*_j + \sum_{k=j}^{i-1}\varepsilon^+_k}, & \quad i \neq j
			                                                 \end{cases} \\
			      \shortintertext{(note that for \(i = 1,\ \varepsilon^- = 0\), since we subtract by 0 - which will never round)}
			               &  &                 & \leq |l_{ij}| \begin{cases}
				                                                    |\varepsilon^/|,                                       & \quad i = j = 1    \\
				                                                    |\varepsilon^-| + |\varepsilon^/|,                     & \quad i = j \neq 1 \\
				                                                    |\varepsilon^*_j| + \sum_{k=j}^{i-1}|\varepsilon^+_k|, & \quad i \neq j
			                                                    \end{cases}   \\
			               &  &                 & \leq |l_{ij}| \begin{cases}
				                                                    eps,        & \quad i = j = 1    \\
				                                                    2eps,       & \quad i = j \neq 1 \\
				                                                    (i-j+1)eps, & \quad i \neq j
			                                                    \end{cases}                                              \\
			               &  &                 & \leq n \cdot eps |l_{ij}|
		      \end{alignat*}
	      \end{proof}

	      % 2
	\item Matrix \(\mat{A}\) is called \emph{strictly column diagonally dominant}, or diagonally dominant for short, if
	      \begin{equation}
		      |a_{ii}| > \sum_{j=1, j \neq i}^{n} |a_{ji}| \label{eq:diagdom}
	      \end{equation}
	      Show that Gaussian elimination with partial pivoting does not actually permute any rows, i.e., that it is identical to Gaussian elimination without pivoting. Hint: Show that after one step of Gaussian elimination, the trailing \((n - 1)\text{-by-}(n - 1)\) submatrix, the \emph{Schur complement} of \(a_{11}\) in \(\mat{A}\), is still diagonally dominant.
	      \begin{proof}
		      For a diagonally dominant matrix \(\mat{A}\), the first step of Gaussian elimination with partial pivoting does not permute any rows, since \(|\alpha_{ii}| > \sum_{j=1, j \neq i}^{n} |\alpha_{ji}|\) and thus \(|\alpha_{ii}| > |\alpha_{ji}|\) for any \(j\neq i\). Then, we have the decomposition for \(\mat{A}\) given by the first step of Gaussian elimination as
		      \begin{align*}
			      \mat{A} & = \mqty[\alpha_{11} & \vec{c}^{\top} \\ \vec{a} & \hat{\mat{A}}]\\
			              & = \mqty[1           & 0              \\ \vec{l} & \mat{I}] \mqty[\alpha_{11} \vec{c}^{\top} \\ 0 & \hat{\mat{A}}-\vec{l}\vec{c}^{\top}] \\
			              & = \mqty[1           & 0              \\ \vec{l} & \mat{I}] \mqty[\alpha_{11} \vec{c}^{\top} \\ 0 & \mat{B}]
		      \end{align*}
		      where \(\vec{l} = \frac{\vec{a}}{\alpha_{11}}\). Note that \(c_i = \alpha_{1,i + 1},\ l_i = \frac{\alpha_{i + 1, 1}}{\alpha_{11}}\). Then, denote the submatrix \(\mat{B}=\hat{\mat{A}}-\vec{l}\vec{c}^{\top}\). Now, it is sufficient to show that \(\mat{B}\) is diagonally dominant, since then we could continue each step of Gaussian elimination recursively always getting a diagonally dominant submatrix. First note
		      \begin{equation}
			      \beta_{ji} = \alpha_{j+1,i+1} - \frac{\alpha_{j+1,1}\alpha_{1,i+1}}{\alpha_{11}} \label{eq:bji}
		      \end{equation}
		      is the expression for the elements in \(\mat{B}\) by definition. Further, from \cref{eq:diagdom} we can split one of the terms out of the sum, and we have
		      \begin{equation}
			      \abs{\alpha_{ii}} - \sum_{\substack{j=1\\j\neq i,k}} \abs{\alpha_{ji}} > \abs{\alpha_{ki}}. \label{eq:diagdomk}
		      \end{equation}
		      We have
		      \begin{align*}
			      \abs{\beta_{ii}} & \stackrel{\mathclap{\eqref{eq:bji}}}{=} \abs{\alpha_{j+1,i+1} - \frac{\alpha_{i+1,1}\alpha_{1,i+1}}{\alpha_{11}}}                \\
			                       & \geq \abs{\alpha_{j+1,i+1}} - \abs{\frac{\alpha_{i+1,1}\alpha_{1,i+1}}{\alpha_{11}}} \qquad\text{by reverse triangle inequality} \\
			                       & \stackrel{\mathclap{\eqref{eq:diagdom}}}{>} \sum_{\substack{j=1                                                                  \\j\neq i+1}}^n \abs{\alpha_{j,i+1}} - \abs{\frac{\alpha_{i+1,1}\alpha_{1,i+1}}{\alpha_{11}}} \\
			                       & \stackrel{???}{>} \sum_{\substack{j=1                                                                                            \\j\neq i+1}}^n \abs{\alpha_{j,i+1}} - \abs{\frac{\alpha_{1,i+1}}{\alpha_{11}}}\qty( \abs{\alpha_{11}} - \sum_{\substack{j=1\\j\neq 1,i+1}}^n \abs{\alpha_{j,1}}) \\
			                       & = \sum_{\substack{j=1                                                                                                            \\j\neq i+1}}^n \abs{a_{j,i+1}}
		      \end{align*}
	      \end{proof}

	      % 3
	\item Let \(\mat{A},\ \mat{B}\), and \(\mat{C}\) be matrices with dimensions such that the product \(\mat{A}^\top\mat{C}\mat{B}^\top\) is well defined. Let \(\mathcal{X}\) be the set of matrices \(\mat{X}\) minimizing \(\|\mat{A}\mat{X} \mat{B} - \mat{C}\|_F\), and let \(\mat{X}_0\) be the unique member of \(\mathcal{X}\) minimizing \(\|\mat{X}\|_F\). Show that \(\mat{X}_0 = \mat{A}^+C\mat{B}^+\). Hint: Use the SVDs of \(\mat{A}\) and \(\mat{B}\).
	      \begin{proof}

	      \end{proof}

	      % 4
	\item Show that the Mooreâ€”Penrose pseudoinverse of \(\mat{A}\) satisfies the following identities:
	      \begin{align*}
		      \mat{A}\mat{A}^+\mat{A}   & = \mat{A},                 \\
		      \mat{A}^+\mat{A}\mat{A}^+ & = \mat{A}^+,               \\
		      \mat{A}^+\mat{A}          & = (\mat{A}^+\mat{A})^\top, \\
		      \mat{A}\mat{A}^+          & = (\mat{A}\mat{A}^+)^\top.
	      \end{align*}
	      \begin{proof}
		      We have
		      \begin{align*}
			      \mat{A}\mat{A}^+\mat{A} & = \mat{A}\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\mat{A}                          \\
			                              & = \mat{A}\cancel{\qty(\mat{A}^{\top}\mat{A})^{-1}}\cancel{\qty(\mat{A}^{\top}\mat{A}) } \\
			                              & = \mat{A}.
		      \end{align*}
		      Then,
		      \begin{align*}
			      \mat{A}^+\mat{A}\mat{A}^+ & = \qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\mat{A}\mat{A}^+                     \\
			                                & = \cancel{\qty(\mat{A}^{\top}\mat{A})^{-1}}\cancel{(\mat{A}^{\top}\mat{A})}\mat{A}^+ \\
			                                & = \mat{A}^+.
		      \end{align*}
		      Further,
		      \begin{align*}
			      \mat{A}^+\mat{A} & = \qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top}\mat{A}                          \\
			                       & = \cancel{\qty(\mat{A}^{\top}\mat{A})^{-1}}\cancel{\qty(\mat{A}^{\top}\mat{A}) } \\
			                       & = \mat{I},
		      \end{align*}
		      and
		      \begin{align*}
			      (\mat{A}^+\mat{A})^\top & = \mat{A}^{\top} (\mat{A}^+)^{\top}                                                \\
			                              & = \mat{A}^{\top} (\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top})^{\top}           \\
			                              & = \mat{A}^{\top} \mat{A} \qty(\qty(\mat{A}^{\top}\mat{A})^{-1})^{\top}             \\
			                              & = \mat{A}^{\top} \mat{A} \qty(\qty(\mat{A}^{\top}\mat{A})^{\top})^{-1}             \\
			                              & = \cancel{\qty(\mat{A}^{\top} \mat{A})} \cancel{\qty(\mat{A}^{\top} \mat{A})^{-1}} \\
			                              & = \mat{I},
		      \end{align*}
		      so \(\mat{A}^+\mat{A}= (\mat{A}^+\mat{A})^\top\).
		      Finally,
		      \begin{align*}
			      (\mat{A}\mat{A}^+)^\top & = \qty(\mat{A}^+)^{\top}\mat{A}^\top                                  \\
			                              & = (\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^{\top})^{\top}\mat{A}^\top \\
			                              & = \mat{A}\qty(\qty(\mat{A}^{\top}\mat{A})^{-1})^{\top}\mat{A}^\top    \\
			                              & = \mat{A}\qty(\qty(\mat{A}^{\top}\mat{A})^{\top})^{-1}\mat{A}^\top    \\
			                              & = \mat{A}\qty(\mat{A}^{\top}\mat{A})^{-1}\mat{A}^\top                 \\
			                              & =\mat{A}\mat{A}^+.
		      \end{align*}
	      \end{proof}

	      % 5
	\item
	      \begin{enumerate}
		      \item Describe a variant of Gaussian elimination that introduces zeros into the columns of \(\mat{A}\) in the order \(n\) : -1 : 2 and which produces the factorization \(\mat{A} = \mat{U}\mat{L}\) where \(\mat{U}\) is the unit upper triangular and \(\mat{L}\) is lower triangular.
		            \begin{answer}
		            \end{answer}

		      \item Based on your algorithm, prove/provide the necessary and sufficient determinant conditions for the existence of the UL decomposition.
		            \begin{answer}
		            \end{answer}

		      \item Write a Matlab code to implement the UL decomposition and apply it to
		            \[
			            \mqty[1&0&2&1\\-4&5&3&-1\\-1&3&1&1\\0&2&0&1]
		            \]
		            to verify that your code generates the required decomposition \(\mat{A} = \mat{U}\mat{L}\).
		            \begin{answer}
		            \end{answer}
	      \end{enumerate}

	      % 6
	\item Even though we rarely need to compute the inverse of a matrix, let us think about it in this problem. Let \(\mat{A} \in \reals^{n\times n}\) be an invertible matrix. Describe an algorithm (based on the LU Decomposition/Gaussian Elimination) that computes \(\mat{A}^{-1}\) with an operation count of \(8n^3/3\) flops (ignoring the lower order terms).
	      \begin{answer}
	      \end{answer}

	      % 7
	\item Inspired by the presentation in [Trefethen and Bau, SIAM Press, 1997], in this problem, we will numerically investigate the growth factor in LU with partial pivoting. In the class (see October 13th notes), we showed that the growth factor \(\rho_{pp}\) could be as large as \(\rho_{pp} = 2n-1\). Indeed we had found an example where this upper bound is attained. However, as we mentioned, the algorithm behaves much better in practice. Here, we will try LU with partial pivoting on random matrices with varying dimensions and plot the observations.

	      Use the command \texttt{n = ceil(logspace(1, 3, 1000))} to create (approximately) logarithmically spaced matrix dimensions between 10 and 1000. Some of the dimensions will be repeated. The variable \texttt{n} is a vector of size 1000 with entries ranging from 10 to 1000. Then, for every entry \texttt{n(i)} of \texttt{n}, i.e., for \(i = 1, 2, \dots , 1000\), create a random matrix \(\mat{A}\) using \texttt{A= randn(n(i), n(i))/sqrt(n(i))}. So, we are creating a random matrix of varying dimensions with normally distributed entries having mean zero and standard deviation \(\sqrt{n(i)}\). Then, compute the growth factor \(\rho_{pp}\) for every \(\mat{A}\). At the end you will have a vector of size 1000 whose entries corresponding to the growth factor for every random \(\mat{A}\). Using the \texttt{loglog} command (logarithmic scale both in the horizontal and 2 vertical axes), plot the growth factor vs the matrix dimensions \texttt{n}. On the same plot, by using the \texttt{hold on} command, plot the growth rate of \(\sqrt{n}\). How is the observed/numerical growth behaving with respect to the theoretical upper bound \(2n-1\) and with respect to \(\sqrt{n}\) ? Comment on your observations.
	      \begin{answer}
	      \end{answer}
\end{enumerate}

\end{document}